{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOC7fW1PuvDNfOIwG+a2Ws"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzsYxJuJfWvu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ðŸ”¹ 1. **Artificial Neural Network (ANN) / Multi-Layer Perceptron (MLP)**\n",
        "- **Description**: The most basic type of feedforward neural network.\n",
        "- **Structure**: Composed of an input layer, one or more hidden layers, and an output layer.\n",
        "- **Use Cases**:\n",
        "  - Classification\n",
        "  - Regression\n",
        "  - Function approximation\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 2. **Convolutional Neural Network (CNN)**\n",
        "- **Description**: Designed for grid-like data (e.g., images).\n",
        "- **Key Features**:\n",
        "  - Convolutional layers\n",
        "  - Pooling layers\n",
        "- **Use Cases**:\n",
        "  - Image classification\n",
        "  - Object detection\n",
        "  - Video analysis\n",
        "  - Medical image analysis\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 3. **Recurrent Neural Network (RNN)**\n",
        "- **Description**: Processes sequences of inputs, maintaining internal memory.\n",
        "- **Variants**:\n",
        "  - Long Short-Term Memory (LSTM)\n",
        "  - Gated Recurrent Unit (GRU)\n",
        "- **Use Cases**:\n",
        "  - Language modeling\n",
        "  - Speech recognition\n",
        "  - Time series forecasting\n",
        "  - Chatbots\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 4. **Autoencoder (AE)**\n",
        "- **Description**: Unsupervised neural network used for learning efficient representations.\n",
        "- **Structure**:\n",
        "  - Encoder: compresses input into a latent representation\n",
        "  - Decoder: reconstructs input from latent code\n",
        "- **Use Cases**:\n",
        "  - Dimensionality reduction\n",
        "  - Anomaly detection\n",
        "  - Denoising\n",
        "  - Feature extraction\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 5. **Variational Autoencoder (VAE)**\n",
        "- **Description**: Probabilistic version of autoencoders; learns latent variable models.\n",
        "- **Use Cases**:\n",
        "  - Generative modeling\n",
        "  - Data generation\n",
        "  - Representation learning\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 6. **Generative Adversarial Network (GAN)**\n",
        "- **Description**: Two networks compete â€” generator and discriminator.\n",
        "- **Use Cases**:\n",
        "  - Image generation\n",
        "  - Style transfer\n",
        "  - Super-resolution\n",
        "  - Deepfakes\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 7. **Transformer**\n",
        "- **Description**: Uses self-attention mechanisms instead of recurrence or convolution.\n",
        "- **Use Cases**:\n",
        "  - Natural Language Processing (NLP)\n",
        "  - Machine translation\n",
        "  - Text summarization\n",
        "  - Vision Transformers (ViT) for image processing\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 8. **Radial Basis Function Network (RBFN)**\n",
        "- **Description**: Uses radial basis functions as activation functions.\n",
        "- **Use Cases**:\n",
        "  - Function approximation\n",
        "  - Time series prediction\n",
        "  - Classification\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 9. **Self-Organizing Map (SOM)**\n",
        "- **Description**: Type of unsupervised network for dimensionality reduction and visualization.\n",
        "- **Use Cases**:\n",
        "  - Clustering\n",
        "  - Visualization of high-dimensional data\n",
        "  - Pattern recognition\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 10. **Hopfield Network**\n",
        "- **Description**: Recurrent network that serves as content-addressable memory.\n",
        "- **Use Cases**:\n",
        "  - Associative memory\n",
        "  - Optimization problems\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 11. **Restricted Boltzmann Machine (RBM)**\n",
        "- **Description**: Undirected probabilistic graphical model used for feature learning.\n",
        "- **Use Cases**:\n",
        "  - Collaborative filtering\n",
        "  - Dimensionality reduction\n",
        "  - Pre-training for deep networks\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 12. **Deep Belief Network (DBN)**\n",
        "- **Description**: Composed of multiple RBMs stacked together.\n",
        "- **Use Cases**:\n",
        "  - Feature extraction\n",
        "  - Unsupervised pre-training\n",
        "  - Handwritten digit recognition\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 13. **Neural Turing Machine (NTM)**\n",
        "- **Description**: Extends neural networks with external memory resources.\n",
        "- **Use Cases**:\n",
        "  - Algorithmic learning\n",
        "  - Memory-intensive tasks\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 14. **Spiking Neural Network (SNN)**\n",
        "- **Description**: Third-generation neural network that mimics biological neurons using spikes.\n",
        "- **Use Cases**:\n",
        "  - Neuromorphic computing\n",
        "  - Energy-efficient AI\n",
        "  - Brain-computer interfaces\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 15. **Recursive Neural Network**\n",
        "- **Description**: Applies same set of weights recursively over a structure (e.g., trees).\n",
        "- **Use Cases**:\n",
        "  - Parsing natural language\n",
        "  - Sentiment analysis\n",
        "  - Semantic parsing\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Neural Network | Use Case | Data Type |\n",
        "|----------------|----------|-----------|\n",
        "| ANN / MLP      | General classification/regression | Tabular |\n",
        "| CNN            | Image processing | Grid (images) |\n",
        "| RNN / LSTM / GRU | Sequence modeling | Time series, text |\n",
        "| Autoencoder    | Feature learning, denoising | Any |\n",
        "| VAE            | Generative modeling | Any |\n",
        "| GAN            | Image generation | Images |\n",
        "| Transformer    | NLP, vision | Text, images |\n",
        "| SOM            | Clustering, visualization | High-dimensional |\n",
        "| Hopfield       | Associative memory | Binary |\n",
        "| RBM            | Feature learning | Binary/Real |\n",
        "| DBN            | Feature learning | Binary/Real |\n",
        "| NTM            | Memory-based computation | Any |\n",
        "| SNN            | Biological modeling | Spatio-temporal |\n",
        "| Recursive NN   | Tree-structured data | Parse trees |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5icAcn33f3mG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F26zYOflf7u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q1: What are Neural Networks in Machine Learning?\n",
        "\n",
        "**A:** Neural Networks (NNs) are computational models inspired by the human brain. They consist of layers of interconnected nodes (neurons) that process and learn patterns from data. They are widely used in tasks like classification, regression, and pattern recognition.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q2: Can you name a few types of Neural Networks and their applications?\n",
        "\n",
        "**A:** Sure! Here are some common types:\n",
        "\n",
        "| Neural Network | Use Case |\n",
        "|----------------|----------|\n",
        "| **ANN / MLP** | General classification and regression problems |\n",
        "| **CNN** | Image classification, object detection |\n",
        "| **RNN / LSTM / GRU** | Sequence modeling (e.g., text, speech, time series) |\n",
        "| **Autoencoder** | Dimensionality reduction, denoising |\n",
        "| **GAN** | Image generation, deepfakes |\n",
        "| **Transformer** | NLP tasks like translation, summarization |\n",
        "| **VAE** | Generative modeling with probabilistic latent space |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q3: What is a CNN and why is it good for images?\n",
        "\n",
        "**A:** Convolutional Neural Networks (CNNs) use convolutional layers to automatically detect spatial features in grid-like data such as images. They're effective because they:\n",
        "- Preserve spatial relationships\n",
        "- Use filters to extract local features (edges, textures, etc.)\n",
        "- Reduce parameters via weight sharing\n",
        "\n",
        "Used in image classification, segmentation, and even video analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q4: What is the difference between RNN and LSTM?\n",
        "\n",
        "**A:**\n",
        "- **RNN**: Basic recurrent networks can model sequences but suffer from **vanishing gradients**, making it hard to learn long-term dependencies.\n",
        "- **LSTM (Long Short-Term Memory)**: A type of RNN with memory cells and gates (input, forget, output), allowing it to remember information over longer sequences.\n",
        "- **GRU**: Similar to LSTM but with fewer gates; simpler and faster.\n",
        "\n",
        "**Use Cases**: Language modeling, speech recognition, time-series forecasting.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q5: What are Autoencoders? What are they used for?\n",
        "\n",
        "**A:** Autoencoders are unsupervised neural networks consisting of:\n",
        "- **Encoder**: Compresses input into a lower-dimensional representation.\n",
        "- **Decoder**: Reconstructs the input from the compressed form.\n",
        "\n",
        "**Applications**:\n",
        "- Denoising\n",
        "- Anomaly detection\n",
        "- Feature extraction\n",
        "- Dimensionality reduction\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q6: Explain GANs and their components.\n",
        "\n",
        "**A:** GANs (Generative Adversarial Networks) have two parts:\n",
        "- **Generator**: Learns to generate fake data that looks real.\n",
        "- **Discriminator**: Learns to distinguish between real and fake data.\n",
        "\n",
        "They train in a competitive setting where the generator tries to fool the discriminator.  \n",
        "**Use Cases**: Image generation, style transfer, data augmentation.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q7: What are Transformers? Why are they better than RNNs?\n",
        "\n",
        "**A:** Transformers use **self-attention mechanisms** to weigh different parts of the input differently. Unlike RNNs, they:\n",
        "- Process all inputs in parallel â†’ faster training\n",
        "- Handle long-range dependencies better\n",
        "- Avoid vanishing gradient issues\n",
        "\n",
        "**Use Cases**: NLP tasks (translation, summarization), Vision Transformers (ViTs)\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q8: What is a Variational Autoencoder (VAE)?\n",
        "\n",
        "**A:** VAEs are probabilistic autoencoders that learn a **latent probability distribution** instead of deterministic codes. They allow generating new data samples similar to the training data.\n",
        "\n",
        "**Use Case**: Generative modeling with structured latent space (e.g., generating faces, digits).\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q9: What are some other specialized NNs?\n",
        "\n",
        "**A:** Some others include:\n",
        "- **Self-Organizing Maps (SOMs)** â€“ For clustering and visualization\n",
        "- **Hopfield Networks** â€“ Associative memory\n",
        "- **Spiking Neural Networks (SNNs)** â€“ Mimic biological neurons for neuromorphic computing\n",
        "- **Recursive Neural Networks** â€“ For tree-structured data like parse trees\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Q10: Which Neural Network would you choose for predicting stock prices?\n",
        "\n",
        "**A:** Stock price prediction involves **time-series data**, so suitable models could be:\n",
        "- **RNN**\n",
        "- **LSTM**\n",
        "- **GRU**\n",
        "- Or more recently, **Transformers** adapted for time-series forecasting\n",
        "\n",
        "These models can capture temporal dependencies effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Summary Tip:\n",
        "Always match the network architecture to the **data structure** and **problem type**:\n",
        "- **Images** â†’ CNN\n",
        "- **Sequences** â†’ RNN/LSTM/GRU or Transformer\n",
        "- **Unsupervised learning** â†’ Autoencoder, GAN, RBM\n",
        "- **Generation** â†’ GAN, VAE\n",
        "- **NLP** â†’ Transformer-based models (BERT, GPT)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GSXrA5rHglz6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "maoksQm1gp9L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}