{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/PX//+YiaQtMsUPW1ayRu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohf9DXwCtjih",
        "outputId": "558e4b4c-681c-4f21-fb32-e862100f2928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained weights: [0.2 0.1]\n",
            "Trained bias: -0.2\n",
            "Input: [0 0] => Predicted: 0\n",
            "Input: [0 1] => Predicted: 0\n",
            "Input: [1 0] => Predicted: 0\n",
            "Input: [1 1] => Predicted: 1\n"
          ]
        }
      ],
      "source": [
        "# Perceptron for AND gate\n",
        "import numpy as np\n",
        "\n",
        "# Input data (4 examples, 2 features each)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Target outputs\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = np.zeros(2)\n",
        "bias = 0\n",
        "lr = 0.1  # learning rate\n",
        "\n",
        "# Activation function\n",
        "def step_function(x):\n",
        "    return 1 if x > 0 else 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for xi, target in zip(X, y):\n",
        "        z = np.dot(weights, xi) + bias\n",
        "        pred = step_function(z)\n",
        "        error = target - pred\n",
        "\n",
        "        # Update rule\n",
        "        weights += lr * error * xi\n",
        "        bias += lr * error\n",
        "\n",
        "print(\"Trained weights:\", weights)\n",
        "print(\"Trained bias:\", bias)\n",
        "\n",
        "# Test\n",
        "for xi in X:\n",
        "    z = np.dot(weights, xi) + bias\n",
        "    print(f\"Input: {xi} => Predicted: {step_function(z)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🔍 1. Core Idea & Intuition\n",
        "\n",
        "> **Perceptron is the simplest type of neural network** that tries to learn from inputs to classify data into two categories (binary classification).\n",
        "> It mimics a biological neuron — takes inputs, applies weights, sums them, passes through an activation function, and gives an output.\n",
        "\n",
        "**Think of it like this:**\n",
        "\n",
        "* You have inputs (like features: marks, height, age)\n",
        "* You assign importance (weights) to each input\n",
        "* You add them up and check if the total is enough (bias + activation)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 2. Structure of a Perceptron\n",
        "\n",
        "**Mathematically:**\n",
        "\n",
        "Given:\n",
        "\n",
        "* Input vector: $X = [x_1, x_2, ..., x_n]$\n",
        "* Weight vector: $W = [w_1, w_2, ..., w_n]$\n",
        "* Bias: $b$\n",
        "* Activation: $y = f(W \\cdot X + b)$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $f$ is a step function: returns 1 if value > 0, else 0.\n",
        "\n",
        "### ✅ Prediction Rule:\n",
        "\n",
        "$$\n",
        "y =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } (W \\cdot X + b) > 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 3. Learning Algorithm (Training)\n",
        "\n",
        "Goal: Adjust weights to minimize errors.\n",
        "\n",
        "For each training sample:\n",
        "\n",
        "1. Predict output $y$\n",
        "2. Calculate error $e = y_{\\text{true}} - y_{\\text{pred}}$\n",
        "3. Update weights:\n",
        "\n",
        "   $$\n",
        "   w_i = w_i + \\eta \\cdot e \\cdot x_i\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   b = b + \\eta \\cdot e\n",
        "   $$\n",
        "\n",
        "Where $\\eta$ is the learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 4. Simple Example\n",
        "\n",
        "Suppose we're building an AND gate:\n",
        "\n",
        "| Input x1 | Input x2 | Output |\n",
        "| -------- | -------- | ------ |\n",
        "| 0        | 0        | 0      |\n",
        "| 0        | 1        | 0      |\n",
        "| 1        | 0        | 0      |\n",
        "| 1        | 1        | 1      |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧑‍💻 5. Minimal Python Code\n",
        "\n",
        "```python\n",
        "# Perceptron for AND gate\n",
        "import numpy as np\n",
        "\n",
        "# Input data (4 examples, 2 features each)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Target outputs\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = np.zeros(2)\n",
        "bias = 0\n",
        "lr = 0.1  # learning rate\n",
        "\n",
        "# Activation function\n",
        "def step_function(x):\n",
        "    return 1 if x > 0 else 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for xi, target in zip(X, y):\n",
        "        z = np.dot(weights, xi) + bias\n",
        "        pred = step_function(z)\n",
        "        error = target - pred\n",
        "\n",
        "        # Update rule\n",
        "        weights += lr * error * xi\n",
        "        bias += lr * error\n",
        "\n",
        "print(\"Trained weights:\", weights)\n",
        "print(\"Trained bias:\", bias)\n",
        "\n",
        "# Test\n",
        "for xi in X:\n",
        "    z = np.dot(weights, xi) + bias\n",
        "    print(f\"Input: {xi} => Predicted: {step_function(z)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 6. Summary\n",
        "\n",
        "| Concept       | Description                                     |\n",
        "| ------------- | ----------------------------------------------- |\n",
        "| Model Type    | Binary Classifier                               |\n",
        "| Core Formula  | $y = f(W \\cdot X + b)$                          |\n",
        "| Learning Rule | $w += \\eta \\cdot (y_{true} - y_{pred}) \\cdot x$ |\n",
        "| Limitation    | Only works with linearly separable data         |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Real Intuition:\n",
        "\n",
        "Imagine you're trying to decide whether to go for a walk:\n",
        "\n",
        "* **Input**: Temperature, Rain, Time of Day\n",
        "* You assign weights to each input\n",
        "* If the final decision score is above a threshold → you go.\n",
        "\n",
        "That’s a perceptron’s logic!\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rVeLetZft20H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np\n",
        "\n",
        "# AND gate dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # inputs\n",
        "y = np.array([0, 0, 0, 1])                     # outputs\n",
        "\n",
        "# Initialize Perceptron model\n",
        "model = Perceptron(max_iter=10, eta0=1.0, random_state=0)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Test the model\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Print results\n",
        "for i, input_data in enumerate(X):\n",
        "    print(f\"Input: {input_data}, Predicted Output: {predictions[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hPNY5fZtxRR",
        "outputId": "a250f747-b224-447e-cf21-2dac5715055f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Predicted Output: 0\n",
            "Input: [0 1], Predicted Output: 0\n",
            "Input: [1 0], Predicted Output: 0\n",
            "Input: [1 1], Predicted Output: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_stochastic_gradient.py:738: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "naTP3V-gul2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}