{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd9d+gcySqKDEIkldrit6U"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M6lr3c112Af"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **neural network** in **deep learning (DL)** is a computational model inspired by how the human brain works. It is used to recognize patterns, learn from data, and make predictions or decisions. Here's the **core idea and all the essentials** you need to understand:\n",
        "\n",
        "---\n",
        "\n",
        "## üß† **Core Idea**\n",
        "\n",
        "A neural network consists of layers of interconnected **neurons (nodes)** that transform input data through a series of mathematical operations. The goal is to **learn the relationship** between inputs and outputs by adjusting internal parameters (called **weights and biases**) during training.\n",
        "\n",
        "---\n",
        "\n",
        "## üìê **Structure of a Neural Network**\n",
        "\n",
        "1. **Input Layer**:\n",
        "   Takes in the raw data (e.g., image pixels, text tokens, numbers).\n",
        "\n",
        "2. **Hidden Layers**:\n",
        "   Perform transformations on the input data using weights, biases, and **activation functions**. Deep networks have **many** such layers (hence \"deep\" learning).\n",
        "\n",
        "3. **Output Layer**:\n",
        "   Produces the final result (e.g., class label, value, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è **How It Works (Step-by-Step)**\n",
        "\n",
        "1. **Forward Propagation**:\n",
        "   Data moves from input ‚Üí hidden layers ‚Üí output. Each neuron computes:\n",
        "\n",
        "   $$\n",
        "   \\text{Output} = \\text{Activation}(\\sum (w_i \\cdot x_i) + b)\n",
        "   $$\n",
        "\n",
        "2. **Loss Calculation**:\n",
        "   The difference between predicted and actual output is calculated using a **loss function** (e.g., mean squared error, cross-entropy).\n",
        "\n",
        "3. **Backward Propagation (Backprop)**:\n",
        "   The network adjusts the weights using **gradient descent** to minimize the loss. This is done via the **chain rule** in calculus.\n",
        "\n",
        "4. **Training Loop**:\n",
        "   This process repeats over many **epochs** (passes over the data) until the model learns well.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ **Key Components**\n",
        "\n",
        "* **Weights and Biases**: Learnable parameters.\n",
        "* **Activation Functions**: Non-linear functions (like ReLU, sigmoid, tanh) that allow the network to learn complex patterns.\n",
        "* **Loss Function**: Measures how well the model is performing.\n",
        "* **Optimizer**: Updates weights to minimize the loss (e.g., SGD, Adam).\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Example Use-Cases\n",
        "\n",
        "* Image recognition (CNNs)\n",
        "* Language translation (RNNs, Transformers)\n",
        "* Fraud detection\n",
        "* Speech recognition\n",
        "* Game playing (Deep Q-Learning)\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Types of Neural Networks\n",
        "\n",
        "* **Feedforward Neural Network (FNN)** ‚Äì basic type, no loops.\n",
        "* **Convolutional Neural Network (CNN)** ‚Äì great for images.\n",
        "* **Recurrent Neural Network (RNN)** ‚Äì good for sequences/time series.\n",
        "* **Transformers** ‚Äì advanced model for NLP and vision tasks.\n",
        "* **Autoencoders** ‚Äì for compression and anomaly detection.\n",
        "* **GANs** ‚Äì for generating data (images, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## üß† In a Nutshell\n",
        "\n",
        "> A neural network learns to approximate functions: given an input, it tries to predict the output by learning from examples. It does so by passing data through layers of neurons and adjusting parameters to reduce error.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "z4O2Y3Lo2DkG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0YH4Id6c2Jzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 1. **Basic Definitions**\n",
        "\n",
        "| Concept         | **Perceptron**                    | **Neural Network**                                              |\n",
        "| --------------- | --------------------------------- | --------------------------------------------------------------- |\n",
        "| **What is it?** | A single-layer binary classifier. | A system of interconnected layers of perceptrons (can be deep). |\n",
        "| **Invented by** | Frank Rosenblatt (1958)           | Evolved over time; deep learning emerged in the 2000s.          |\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è 2. **Structure**\n",
        "\n",
        "| Aspect            | **Perceptron**                       | **Neural Network**                                |\n",
        "| ----------------- | ------------------------------------ | ------------------------------------------------- |\n",
        "| **Layers**        | Only **one layer** (input ‚Üí output). | Multiple layers (input ‚Üí hidden layers ‚Üí output). |\n",
        "| **Complexity**    | Simple, linear model.                | Can be very deep and complex.                     |\n",
        "| **Hidden Layers** | ‚ùå No hidden layers.                  | ‚úÖ One or more hidden layers.                      |\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ 3. **Mathematical Power**\n",
        "\n",
        "| Feature                   | **Perceptron**                         | **Neural Network**                  |\n",
        "| ------------------------- | -------------------------------------- | ----------------------------------- |\n",
        "| **Function it can learn** | Only **linearly separable** functions. | Can learn **non-linear** functions. |\n",
        "| **Examples**              | Can‚Äôt solve XOR problem.               | Can solve XOR and much more.        |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è 4. **Activation Function**\n",
        "\n",
        "| Feature         | **Perceptron**                  | **Neural Network**                                     |\n",
        "| --------------- | ------------------------------- | ------------------------------------------------------ |\n",
        "| **Activation**  | Step function (binary output).  | Uses non-linear functions (ReLU, sigmoid, tanh, etc.). |\n",
        "| **Output Type** | 0 or 1 (binary classification). | Can output continuous values or multiple classes.      |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ 5. **Learning & Training**\n",
        "\n",
        "| Feature           | **Perceptron**                       | **Neural Network**                             |\n",
        "| ----------------- | ------------------------------------ | ---------------------------------------------- |\n",
        "| **Training Rule** | Simple Perceptron Learning Rule.     | Backpropagation + Optimizers (like SGD, Adam). |\n",
        "| **Loss Function** | Usually simple (e.g., binary error). | More sophisticated (cross-entropy, MSE, etc.). |\n",
        "\n",
        "---\n",
        "\n",
        "## üìä 6. **Usage Today**\n",
        "\n",
        "| Feature         | **Perceptron**                             | **Neural Network**                                        |\n",
        "| --------------- | ------------------------------------------ | --------------------------------------------------------- |\n",
        "| **Modern Use**  | Mostly educational or historical interest. | Widely used in AI applications today (vision, NLP, etc.). |\n",
        "| **Scalability** | Not scalable.                              | Highly scalable (deep networks, GPUs, etc.).              |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Table\n",
        "\n",
        "| Feature            | **Perceptron**          | **Neural Network**      |\n",
        "| ------------------ | ----------------------- | ----------------------- |\n",
        "| Layers             | One                     | Multiple                |\n",
        "| Hidden Layer       | ‚ùå No                    | ‚úÖ Yes                   |\n",
        "| Output             | Binary (0/1)            | Flexible                |\n",
        "| Problem Solving    | Linearly separable only | Non-linear problems too |\n",
        "| Learning Algorithm | Perceptron rule         | Backpropagation         |\n",
        "| Use Today          | Rare                    | Very common             |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WW02_Mpe2PP0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifPkrOXd2S6A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}